{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c756704-c0f4-4e2e-850f-1d23992fffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110e00f-0ed7-4214-bf63-452909071c60",
   "metadata": {},
   "source": [
    "Before running this notebook, the pre-processed Enron-Spam dataset must be downloaded from http://www2.aueb.gr/users/ion/data/enron-spam/\n",
    "Each file downloaded must be extracted into the data/Enron-Spam folder in the project directory, which should contain folders named enron1-6.\n",
    "\n",
    "We do a little processing on the dataset to get it into a more suitable form:\n",
    "* Convert all text to lowercase\n",
    "* Strip all stop words\n",
    "* Strip all words with <3 characters (possibly unnecessary given the previous step?)\n",
    "* Strip all punctuation (maybe also numerals?)\n",
    "* Throw out all forward and reply emails (spam is not generally interacted with, so these aren't interesting for us)\n",
    "\n",
    "First thing we need to do is load all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7315a412-3ebd-4b56-b2b7-85cd4b20161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../data/Enron-Spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d1ca9-ddee-4bfe-b1b3-e4390d72d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = []\n",
    "isspam = []\n",
    "folders = os.listdir()\n",
    "for folder in folders:\n",
    "    hamnames = os.listdir(os.path.join(folder, 'ham'))\n",
    "    spamnames = os.listdir(os.path.join(folder, 'spam'))\n",
    "    for hamname in hamnames:\n",
    "        with open(os.path.join(folder, 'ham', hamname), errors='ignore') as hamfile:\n",
    "            emails.append(hamfile.read())\n",
    "            isspam.append(False)\n",
    "    for spamname in spamnames:\n",
    "        with open(os.path.join(folder, 'spam', spamname), errors='ignore') as spamfile:\n",
    "            emails.append(spamfile.read())\n",
    "            isspam.append(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af9c72-e9d6-4448-9ef6-296b001b42da",
   "metadata": {},
   "source": [
    "Now we put this data into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3b34d-4b52-4d94-b6da-5267229acb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Email' : emails, 'Spam' : isspam})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae92bd9-4efa-41dd-8a46-51af999979b5",
   "metadata": {},
   "source": [
    "It is now time to process the data. The first thing we do is to throw out forwards and replies so as to avoid having to process these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0703c2-0d52-4214-b163-bb8c4ae29d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replyorforward(email): # Note that forwards come in two different forms in this dataset, so we must recognise both\n",
    "    return (\"re :\" in email) or (\"fw :\" in email) or (\"- - - - - - - - - - - - - - - - - - - - - - forwarded\" in email)\n",
    "\n",
    "data.drop(data[list(map(replyorforward, data.Email))].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd95c9-3ac3-48b6-ad10-4321cb36b839",
   "metadata": {},
   "source": [
    "Now we clean up and tokenize the remaining entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe053c-24e8-4f8d-bdab-457e7850fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english')) # We use the list of stop words provided by the nltk library\n",
    "loweremail = map(lambda x: x.lower(), data.Email)\n",
    "tokenemail = map(word_tokenize, loweremail)\n",
    "taggedemail = map(pos_tag, tokenemail) # Adds part-of-speech tags to each word - these aren't needed yet, but they will be later\n",
    "\n",
    "def strip(email): # Takes a tokenized email and strips it of stopwords and tokens with length < 3 (which will include punctuation for free)\n",
    "    return [word for word in email if ((word[0] not in stop) and (len(word[0]) > 2))]\n",
    "\n",
    "strippedemail = list(map(lambda x:strip(x)[1:], taggedemail)) # we also drop the first token, since this is always 'subject'\n",
    "\n",
    "data.Email = list(map(lambda x: list(map(lambda y: y[0], x)), strippedemail))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea3fad-b22a-4553-bdf2-e6bfde6ee039",
   "metadata": {},
   "source": [
    "Now we apply lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c713ac-a8a2-4c65-b2af-2060d51ef465",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def lemmatize(taggedword):\n",
    "    word, tag = taggedword\n",
    "    if tag[0] == 'N': # word is a noun\n",
    "        lemma = wnl.lemmatize(word, 'n')\n",
    "    elif tag[0] == 'V': # word is a verb\n",
    "        lemma = wnl.lemmatize(word, 'v')\n",
    "    elif tag[0] == 'J': # word is an adjective\n",
    "        lemma = wnl.lemmatize(word, 'a')\n",
    "    elif tag[0] == 'R': # word is an adverb\n",
    "        lemma = wnl.lemmatize(word, 'r')\n",
    "    else: # word is not lemmatizable\n",
    "        lemma = word\n",
    "    return lemma\n",
    "lemmatizedemail = map(lambda x: list(map(lemmatize, x)), strippedemail)\n",
    "\n",
    "def stripnotag(email):\n",
    "    return [word for word in email if ((word not in stop) and (len(word) > 2))]\n",
    "\n",
    "strippedlemmatized = map(stripnotag, lemmatizedemail) # we strip again, to remove short/stop word lemmas\n",
    "\n",
    "data.insert(1, 'Lemmatized', list(strippedlemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95806cc0-62e6-422c-a5cd-5d8e88eeef2e",
   "metadata": {},
   "source": [
    "Finally, we write out the processed data to a csv file and reset the indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277bcac-f3f8-47c7-9173-43870d607d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "data.to_csv(\"../data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4c70b-1208-483c-894f-69950f722a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
