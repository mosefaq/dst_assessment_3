{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c756704-c0f4-4e2e-850f-1d23992fffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110e00f-0ed7-4214-bf63-452909071c60",
   "metadata": {},
   "source": [
    "We do a little processing on the dataset to get it into a more suitable form:\n",
    "* Convert all text to lowercase\n",
    "* Strip all stop words\n",
    "* Strip all words with <3 characters (possibly unnecessary given the previous step?)\n",
    "* Strip all punctuation (maybe also numerals?)\n",
    "* Throw out all forward and reply emails (spam is not generally interacted with, so these aren't interesting for us)\n",
    "\n",
    "First thing we need to do is load all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7315a412-3ebd-4b56-b2b7-85cd4b20161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"data/Enron-Spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9d1ca9-ddee-4bfe-b1b3-e4390d72d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = []\n",
    "isspam = []\n",
    "folders = os.listdir()\n",
    "for folder in folders:\n",
    "    hamnames = os.listdir(os.path.join(folder, 'ham'))\n",
    "    spamnames = os.listdir(os.path.join(folder, 'spam'))\n",
    "    for hamname in hamnames:\n",
    "        with open(os.path.join(folder, 'ham', hamname), errors='ignore') as hamfile:\n",
    "            emails.append(hamfile.read())\n",
    "            isspam.append(False)\n",
    "    for spamname in spamnames:\n",
    "        with open(os.path.join(folder, 'spam', spamname), errors='ignore') as spamfile:\n",
    "            emails.append(spamfile.read())\n",
    "            isspam.append(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af9c72-e9d6-4448-9ef6-296b001b42da",
   "metadata": {},
   "source": [
    "Now we put this data into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d3b34d-4b52-4d94-b6da-5267229acb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Email' : emails, 'Spam' : isspam})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae92bd9-4efa-41dd-8a46-51af999979b5",
   "metadata": {},
   "source": [
    "It is now time to process the data. The first thing we do is to throw out forwards and replies so as to avoid having to process these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0703c2-0d52-4214-b163-bb8c4ae29d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replyorforward(email): # Note that forwards come in two different forms in this dataset, so we must recognise both\n",
    "    return (\"re :\" in email) or (\"fw :\" in email) or (\"- - - - - - - - - - - - - - - - - - - - - - forwarded\" in email)\n",
    "\n",
    "data.drop(data[list(map(replyorforward, data.Email))].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd95c9-3ac3-48b6-ad10-4321cb36b839",
   "metadata": {},
   "source": [
    "Now we clean up and tokenize the remaining entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24fe053c-24e8-4f8d-bdab-457e7850fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english')) # We use the list of stop words provided by the nltk library\n",
    "loweremail = map(lambda x: x.lower(), data.Email)\n",
    "tokenemail = map(tokenize.word_tokenize, loweremail)\n",
    "\n",
    "def strip(email): # Takes a tokenized email and strips it of stopwords and tokens with length < 3 (which will include punctuation for free)\n",
    "    return [word for word in email if ((word not in stop) and (len(word) > 2))]\n",
    "\n",
    "strippedemail = map(lambda x:strip(x)[1:], tokenemail) # we also drop the first token, since this is always 'subject'\n",
    "\n",
    "data.Email = list(strippedemail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95806cc0-62e6-422c-a5cd-5d8e88eeef2e",
   "metadata": {},
   "source": [
    "Finally, we write out the processed data to a csv file and reset the indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277bcac-f3f8-47c7-9173-43870d607d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "data.to_csv(\"../data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
